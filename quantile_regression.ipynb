{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import properscoring as prscore\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess  the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('power_weather_data.csv')\n",
    "\n",
    "# csv file MUST contain 'date' and 'Power' fields\n",
    "# optional: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].apply(lambda x: x.hour )\n",
    "df['month'] = df['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = df['Power']\n",
    "\n",
    "PowerData = pd.concat([P.shift(3), P.shift(2), P.shift(1)], axis=1)\n",
    "PowerData.columns = ['t-45', 't-30', 't-15']\n",
    "\n",
    "df = pd.concat([df, PowerData.reindex(df.index)], axis=1)\n",
    "    \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = [['2018-03-01', '2019-03-15']]\n",
    "\n",
    "val_days = 14\n",
    "\n",
    "n_points_day = 4 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for w in weeks:\n",
    "    \n",
    "    w_start = datetime.strptime(w[0]+\" 00:00\", '%Y-%m-%d %H:%M')\n",
    "    w_end = datetime.strptime(w[1]+\" 23:59\", '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    dfs.append(df[(df['date'] > w_start) & (df['date'] < w_end)])\n",
    "    \n",
    "n_sets = len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = []\n",
    "X_test_ = []\n",
    "y_train_ = []\n",
    "y_test_ = []\n",
    "\n",
    "x_scaler = []\n",
    "y_scaler = []\n",
    "\n",
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "for i in range(n_sets):\n",
    "\n",
    "    train = dfs[i][:int(-n_points_day*val_days)]\n",
    "    test = dfs[i][int(-n_points_day*val_days):]\n",
    "    \n",
    "    X_tr = train.drop(['Power','date'], axis=1).values\n",
    "    X_t = test.drop(['Power','date'], axis=1).values\n",
    "    \n",
    "    y_tr = train['Power'].values\n",
    "    y_t = test['Power'].values\n",
    "    \n",
    "    x_sc = MinMaxScaler()\n",
    "    y_sc = MinMaxScaler()\n",
    "#     x_sc = StandardScaler()\n",
    "#     y_sc = StandardScaler()\n",
    "    x_sc.fit(X_tr)\n",
    "    y_sc.fit(y_tr.reshape(-1, 1))  #reshape only because fit needs a 2d array\n",
    "    x_scaler.append(x_sc)\n",
    "    y_scaler.append(y_sc)\n",
    "    \n",
    "    X_train_.append(x_sc.transform(X_tr))\n",
    "    X_test_.append(x_sc.transform(X_t))\n",
    "    y_train_.append(y_sc.transform(y_tr.reshape(-1, 1)))\n",
    "    y_test_.append(y_sc.transform(y_t.reshape(-1, 1)))\n",
    "    \n",
    "    t_train.append(dfs[i].iloc[:int(-n_points_day*val_days)]['date'].values)\n",
    "    t_test.append(dfs[i].iloc[int(-n_points_day*val_days):]['date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_\n",
    "X_test = X_test_\n",
    "y_train = y_train_\n",
    "y_test = y_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_50 = []\n",
    "models_2_5 = []\n",
    "models_97_5 = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(n_sets):\n",
    "    model_50 = sm.QuantReg(y_train[i], X_train[i]).fit(q=0.5)\n",
    "    model_2_5 = sm.QuantReg(y_train[i], X_train[i]).fit(q=0.025)\n",
    "    model_97_5 = sm.QuantReg(y_train[i], X_train[i]).fit(q=0.975)\n",
    "    \n",
    "    models_50.append(model_50)\n",
    "    models_2_5.append(model_2_5)\n",
    "    models_97_5.append(model_97_5)\n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/n_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PICP_func(y, lower, upper):\n",
    "    sum_points = 0\n",
    "    for i, yi in enumerate(y):\n",
    "        if lower[i] <= yi <= upper[i]:\n",
    "            sum_points += 1\n",
    "    \n",
    "    return sum_points / len(y)\n",
    "\n",
    "def PINAW_func(y, lower, upper):\n",
    "    PIAW = np.mean(upper - lower)\n",
    "    R = np.max(y) - np.min(y)\n",
    "    PINAW = PIAW / R\n",
    "    \n",
    "    return PINAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "y_hat = []\n",
    "upper_hat = []\n",
    "lower_hat = []\n",
    "\n",
    "MAE_all = []\n",
    "RMSE_all = []\n",
    "MBE_all = []\n",
    "CRPS_all = []\n",
    "\n",
    "for i in range(n_sets):\n",
    "    \n",
    "    model_50 = models_50[i]\n",
    "    model_2_5 = models_2_5[i]\n",
    "    model_97_5 = models_97_5[i]\n",
    "    \n",
    "    X_test_i = X_test[i]\n",
    "    y_test_i = y_test[i]\n",
    "    \n",
    "    # For multi-step ahead prediction\n",
    "    y_first = model_50.predict(X_test_i[:3])\n",
    "    \n",
    "    y_3 = y_first[3-3]\n",
    "    y_2 = y_first[3-2]\n",
    "    y_1 = y_first[3-1]\n",
    "    for j in range(3, X_test[i].shape[0]):\n",
    "        X_test_i[j][-3] = y_3\n",
    "        X_test_i[j][-2] = y_2\n",
    "        X_test_i[j][-1] = y_1\n",
    "        y_pred_j = model_50.predict(X_test_i[j])\n",
    "        y_3 = y_2\n",
    "        y_2 = y_1\n",
    "        y_1 = y_pred_j\n",
    "    # end of multi-step ahead\n",
    "    \n",
    "    mean = model_50.predict(X_test_i)\n",
    "    lower = model_2_5.predict(X_test_i)\n",
    "    upper = model_97_5.predict(X_test_i)\n",
    "    \n",
    "    mean = y_scaler[i].inverse_transform(mean.reshape(1, -1))\n",
    "    lower = y_scaler[i].inverse_transform(lower.reshape(1, -1))\n",
    "    upper = y_scaler[i].inverse_transform(upper.reshape(1, -1))\n",
    "    \n",
    "    mean = mean.flatten()\n",
    "    lower = lower.flatten()\n",
    "    upper = upper.flatten()\n",
    "    \n",
    "    real_y_test = y_scaler[i].inverse_transform(y_test_i)\n",
    "    real_y_test = real_y_test.flatten()\n",
    "    \n",
    "    \n",
    "    y_hat.append(mean)\n",
    "    y.append(real_y_test)\n",
    "    lower_hat.append(lower)\n",
    "    upper_hat.append(upper)\n",
    "    \n",
    "    # Deterministic metrics\n",
    "    MAE = mean_absolute_error(real_y_test, mean)\n",
    "    RMSE = mean_squared_error(real_y_test, mean, squared=False)\n",
    "    MBE = np.mean(mean - real_y_test)\n",
    "    print(f'MAE: {MAE:.3f}')\n",
    "    print(f'RMSE: {RMSE:.3f}')\n",
    "    print(f'MBE: {MBE:.3f}')\n",
    "    \n",
    "    # Probabilistic metrics\n",
    "    PICP = PICP_func(real_y_test, lower, upper)\n",
    "    PINAW = PINAW_func(real_y_test, lower, upper)\n",
    "    C = prscore.crps_gaussian(real_y_test, mu=mean, sig=((upper-lower)/4))\n",
    "    CRPS = C.mean()\n",
    "    print(f'PICP: {PICP:.3f}')\n",
    "    print(f'PINAW: {PINAW:.3f}')\n",
    "    print(f'CRPS: {CRPS:.3f}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
