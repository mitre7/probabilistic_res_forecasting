{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import concat\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel as RBF\n",
    "# from gpytorch.kernels import ScaleKernel as C\n",
    "from gpytorch.kernels import PeriodicKernel as Per\n",
    "from gpytorch.kernels import RQKernel as RQ\n",
    "from gpytorch.kernels import MaternKernel as M\n",
    "from gpytorch.kernels import PolynomialKernel\n",
    "\n",
    "import time\n",
    "\n",
    "import properscoring as prscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess  the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('power_weather_data.csv')\n",
    "\n",
    "# csv file MUST contain 'date' and 'Power' fields\n",
    "# optional: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].apply(lambda x: x.hour )\n",
    "df['month'] = df['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = df['Power']\n",
    "\n",
    "PowerData = pd.concat([P.shift(3), P.shift(2), P.shift(1)], axis=1)\n",
    "PowerData.columns = ['t-45', 't-30', 't-15']\n",
    "\n",
    "df = pd.concat([df, PowerData.reindex(df.index)], axis=1)\n",
    "    \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = [['2018-03-01', '2019-03-15']]\n",
    "\n",
    "val_days = 14\n",
    "\n",
    "n_points_day = 4 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for w in weeks:\n",
    "    \n",
    "    w_start = datetime.strptime(w[0]+\" 00:00\", '%Y-%m-%d %H:%M')\n",
    "    w_end = datetime.strptime(w[1]+\" 23:59\", '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    dfs.append(df[(df['date'] > w_start) & (df['date'] < w_end)])\n",
    "    \n",
    "n_sets = len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = []\n",
    "X_test_ = []\n",
    "y_train_ = []\n",
    "y_test = []\n",
    "\n",
    "x_scaler = []\n",
    "y_scaler = []\n",
    "\n",
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    train = dfs[i][:int(-n_points_day*val_days)]\n",
    "    test = dfs[i][int(-n_points_day*val_days):]\n",
    "    \n",
    "    X_tr = train.drop(['Power','Time'], axis=1).values\n",
    "    X_t = test.drop(['Power','Time'], axis=1).values\n",
    "    \n",
    "    y_tr = train['Power'].values\n",
    "    y_t = test['Power'].values\n",
    "    \n",
    "    x_sc = MinMaxScaler()\n",
    "    y_sc = MinMaxScaler(feature_range=(-1,1))\n",
    "#     x_sc = StandardScaler()\n",
    "#     y_sc = StandardScaler()\n",
    "    x_sc.fit(X_tr)\n",
    "    y_sc.fit(y_tr.reshape(-1, 1))\n",
    "    x_scaler.append(x_sc)\n",
    "    y_scaler.append(y_sc)\n",
    "    \n",
    "    X_train_.append(x_sc.transform(X_tr))\n",
    "    X_test_.append(x_sc.transform(X_t))\n",
    "    y_train_.append(y_sc.transform(y_tr.reshape(-1, 1)))\n",
    "    y_test.append(y_t)\n",
    "    \n",
    "    t_train.append(dfs[i].iloc[:int(-n_points_day*val_days)]['Time'].values)\n",
    "    t_test.append(dfs[i].iloc[int(-n_points_day*val_days):]['Time'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    X_train.append(torch.from_numpy(X_train_[i]))\n",
    "    X_test.append(torch.from_numpy(X_test_[i]))\n",
    "    \n",
    "    y_tr = torch.from_numpy(y_train_[i])\n",
    "    y_train.append(torch.flatten(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X_train, y_train, likelihood):\n",
    "        super(ExactGPModel, self).__init__(X_train, y_train, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = C(RBF())      \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_iter = 100\n",
    "train_loss = []\n",
    "\n",
    "models = []\n",
    "likelihoods = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    \n",
    "    print(i)\n",
    "    X_tr = X_train[i]\n",
    "    y_tr = y_train[i]\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(X_tr, y_tr, likelihood)\n",
    "\n",
    "    model = model.double()\n",
    "    likelihood = likelihood.double()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.07) \n",
    "\n",
    "    # Loss for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    ite = []\n",
    "    loss_all = []\n",
    "    \n",
    "    for j in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X_tr)\n",
    "        # Calculate loss and backprop gradients\n",
    "        loss = -mll(output, y_tr)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        ite = np.append(ite, j)\n",
    "        loss_all = np.append(loss_all, loss.detach().numpy())\n",
    "        \n",
    "    \n",
    "    train_loss.append(loss_all)\n",
    "    models.append(model)\n",
    "    likelihoods.append(likelihood)\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PICP_func(y, lower, upper):\n",
    "    sum_points = 0\n",
    "    for i, yi in enumerate(y):\n",
    "        if lower[i] <= yi <= upper[i]:\n",
    "            sum_points += 1\n",
    "    \n",
    "    return sum_points / len(y)\n",
    "\n",
    "def PINAW_func(y, lower, upper):\n",
    "    PIAW = np.mean(upper - lower)\n",
    "    R = np.max(y) - np.min(y)\n",
    "    PINAW = PIAW / R\n",
    "    \n",
    "    return PINAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_all = []\n",
    "CRPS_all = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # Unpacking\n",
    "    model = models[i]\n",
    "    likelihood = likelihoods[i]\n",
    "    X_t = X_test[i]\n",
    "    y_t = y_test[i]\n",
    "    x_sc = x_scaler[i]\n",
    "    y_sc = y_scaler[i]\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # For multi-step ahead prediction\n",
    "    y_45 = model(X_t[0].unsqueeze(0)).mean\n",
    "    y_30 = model(X_t[1].unsqueeze(0)).mean\n",
    "    y_15 = model(X_t[2].unsqueeze(0)).mean\n",
    "    for j in range(3, X_t.shape[0]):\n",
    "        X_t[j][-3] = y_45\n",
    "        X_t[j][-2] = y_30\n",
    "        X_t[j][-1] = y_15\n",
    "        y_pred_j = model(X_t[j].unsqueeze(0))\n",
    "        y_45 = y_30\n",
    "        y_30 = y_15\n",
    "        y_15 = y_pred_j.mean\n",
    "    # end of multi-step ahead\n",
    "    \n",
    "    y_pred_i = model(X_t)\n",
    "    f_pred_i = likelihood(model(X_t))\n",
    "    \n",
    "    y_pred = y_pred_i.mean\n",
    "    y_var = y_pred_i.variance\n",
    "    y_covar = y_pred_i.covariance_matrix\n",
    "    \n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    \n",
    "    real_y_pred = y_sc.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    \n",
    "    real_y_pred = real_y_pred.flatten()\n",
    "    real_y_test = y_t.flatten()\n",
    "    \n",
    "    lower, upper = f_pred_i.confidence_region()\n",
    "    \n",
    "    lower = lower.detach().numpy()\n",
    "    upper = upper.detach().numpy()\n",
    "    \n",
    "    lower = y_sc.inverse_transform(lower.reshape(-1, 1))\n",
    "    upper = y_sc.inverse_transform(upper.reshape(-1, 1))\n",
    "    \n",
    "    lower = lower.flatten()\n",
    "    upper = upper.flatten()\n",
    "    \n",
    "    mean = (upper+lower)/2\n",
    "    std = (mean - lower)/1.96\n",
    "\n",
    "    # Deterministic metrics\n",
    "    MAE = mean_absolute_error(real_y_test, mean)\n",
    "    RMSE = mean_squared_error(real_y_test, mean, squared=False)\n",
    "    MBE = np.mean(mean - real_y_test)\n",
    "    print(f'MAE: {MAE:.3f}')\n",
    "    print(f'RMSE: {RMSE:.3f}')\n",
    "    print(f'MBE: {MBE:.3f}')\n",
    "    \n",
    "    # Probabilistic metrics\n",
    "    PICP = PICP_func(real_y_test, lower, upper)\n",
    "    PINAW = PINAW_func(real_y_test, lower, upper)\n",
    "    C = prscore.crps_gaussian(real_y_test, mu=mean, sig=std)\n",
    "    CRPS = C.mean()\n",
    "    print(f'PICP: {PICP:.3f}')\n",
    "    print(f'PINAW: {PINAW:.3f}')\n",
    "    print(f'CRPS: {CRPS:.3f}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
