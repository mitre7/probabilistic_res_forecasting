{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "import time\n",
    "\n",
    "import properscoring as prscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess  the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('power_weather_data.csv')\n",
    "\n",
    "# csv file MUST contain 'date' and 'Power' fields\n",
    "# optional: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].apply(lambda x: x.hour )\n",
    "df['month'] = df['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = df['Power']\n",
    "\n",
    "PowerData = pd.concat([P.shift(3), P.shift(2), P.shift(1)], axis=1)\n",
    "PowerData.columns = ['t-45', 't-30', 't-15']\n",
    "\n",
    "df = pd.concat([df, PowerData.reindex(df.index)], axis=1)\n",
    "    \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = [['2018-03-01', '2019-03-15']]\n",
    "\n",
    "val_days = 14\n",
    "\n",
    "n_points_day = 4 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for w in weeks:\n",
    "    \n",
    "    w_start = datetime.strptime(w[0]+\" 00:00\", '%Y-%m-%d %H:%M')\n",
    "    w_end = datetime.strptime(w[1]+\" 23:59\", '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    dfs.append(df[(df['date'] > w_start) & (df['date'] < w_end)])\n",
    "    \n",
    "n_sets = len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = []\n",
    "X_test_ = []\n",
    "y_train_ = []\n",
    "y_test_ = []\n",
    "\n",
    "x_scaler = []\n",
    "y_scaler = []\n",
    "\n",
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    train = dfs[i][:int(-n_points_day*val_days)]\n",
    "    test = dfs[i][int(-n_points_day*val_days):]\n",
    "    \n",
    "    X_tr = train.drop(['Power','Time'], axis=1).values\n",
    "    X_t = test.drop(['Power','Time'], axis=1).values\n",
    "    \n",
    "    y_tr = train['Power'].values\n",
    "    y_t = test['Power'].values\n",
    "    \n",
    "    x_sc = MinMaxScaler()\n",
    "    y_sc = MinMaxScaler()\n",
    "#     x_sc = StandardScaler()\n",
    "#     y_sc = StandardScaler()\n",
    "    x_sc.fit(X_tr)\n",
    "    y_sc.fit(y_tr.reshape(-1, 1))\n",
    "    x_scaler.append(x_sc)\n",
    "    y_scaler.append(y_sc)\n",
    "    \n",
    "    X_train_.append(x_sc.transform(X_tr))\n",
    "    X_test_.append(x_sc.transform(X_t))\n",
    "    y_train_.append(y_sc.transform(y_tr.reshape(-1, 1)) + 0.001)\n",
    "    y_test_.append(y_sc.transform(y_t.reshape(-1, 1)) + 0.001)\n",
    "    \n",
    "    t_train.append(dfs[i].iloc[:int(-n_points_day*val_days)]['Time'].values)\n",
    "    t_test.append(dfs[i].iloc[int(-n_points_day*val_days):]['Time'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    X_train.append(torch.from_numpy(X_train_[i]).float())\n",
    "    X_test.append(torch.from_numpy(X_test_[i]).float())\n",
    "    \n",
    "    y_tr = torch.from_numpy(y_train_[i]).float()\n",
    "    y_train.append(torch.squeeze(y_tr))\n",
    "    y_t = torch.from_numpy(y_test_[i]).float()\n",
    "    y_test.append(torch.squeeze(y_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_neurons = 50\n",
    "eta = 30\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, n_features):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "    self.fc2 = nn.Linear(n_neurons, 2)\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x)) #\n",
    "    return torch.sigmoid(self.fc2(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CWC(y_pred, y_true):\n",
    "    \n",
    "    y_pred = Variable(y_pred, requires_grad=True).to(device)\n",
    "    y_true = Variable(y_true, requires_grad=True).to(device)\n",
    "    \n",
    "    u = y_pred.detach().numpy().T[0]\n",
    "    l = y_pred.detach().numpy().T[1]\n",
    "    \n",
    "    u = torch.squeeze(torch.from_numpy(u).float())\n",
    "    l = torch.squeeze(torch.from_numpy(l).float())\n",
    "   \n",
    "    sum = 0\n",
    "    W = []\n",
    "    for i in range(len(y_pred)):\n",
    "        \n",
    "        Wi = torch.abs(u[i]-l[i]) #)**2 \n",
    "        W.append(Wi)\n",
    "        \n",
    "        if l[i] < y_true[i] < u[i]:\n",
    "            sum += 1\n",
    "    \n",
    "    #calculate PICP: PI coverage probability\n",
    "    PICP = sum/len(y_true)\n",
    "    \n",
    "    #calculate MPIW\n",
    "    W = np.array(W)\n",
    "    W = torch.from_numpy(W).float()\n",
    "    MPIW = torch.sqrt(torch.mean(W))\n",
    "    \n",
    "    R = torch.max(y_true)-torch.min(y_true)\n",
    "\n",
    "    return ((MPIW)/R)*(1+1*math.exp(-eta*(PICP-0.95)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler(object):\n",
    "    def __init__(self, minval, maxval, dtype='float', cuda=False):\n",
    "        self.minval = minval\n",
    "        self.maxval = maxval\n",
    "        self.cuda = cuda\n",
    "        self.dtype_str = dtype\n",
    "        dtypes = {\n",
    "            'float': torch.cuda.FloatTensor if cuda else torch.FloatTensor,\n",
    "            'int': torch.cuda.IntTensor if cuda else torch.IntTensor,\n",
    "            'long': torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        }\n",
    "        self.dtype = dtypes[dtype]\n",
    "\n",
    "    def sample(self, size):\n",
    "        if self.dtype_str == 'float':\n",
    "            return self.dtype(*size).uniform_(\n",
    "                self.minval, self.maxval\n",
    "            )\n",
    "        elif self.dtype_str == 'int' or self.dtype_str == 'long':\n",
    "            return self.dtype(*size).random_(\n",
    "                self.minval, self.maxval + 1\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"unknown dtype\")\n",
    "\n",
    "\n",
    "class GaussianSampler(object):\n",
    "    def __init__(self, mu, sigma, dtype='float', cuda=False):\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu\n",
    "        self.cuda = cuda\n",
    "        self.dtype_str = dtype\n",
    "        dtypes = {\n",
    "            'float': torch.cuda.FloatTensor if cuda else torch.FloatTensor,\n",
    "            'int': torch.cuda.IntTensor if cuda else torch.IntTensor,\n",
    "            'long': torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "        }\n",
    "        self.dtype = dtypes[dtype]\n",
    "\n",
    "    def sample(self, size):\n",
    "        ''' pytorch doesnt support int or long normal distrs\n",
    "            so we will resolve to casting '''\n",
    "        rand_float = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        rand_block = rand_float(*size).normal_(self.mu, self.sigma)\n",
    "\n",
    "        if self.dtype_str == 'int' or self.dtype_str == 'long':\n",
    "            rand_block = rand_block.type(self.dtype)\n",
    "\n",
    "        return rand_block\n",
    "\n",
    "\n",
    "class SimulatedAnnealing(Optimizer):\n",
    "    def __init__(self, params, sampler, tau0=5.0, anneal_rate=0.0003,\n",
    "                 min_temp=1e-5, anneal_every=10, hard=True, hard_rate=0.95):\n",
    "        defaults = dict(sampler=sampler, tau0=tau0, tau=tau0, anneal_rate=anneal_rate,\n",
    "                        min_temp=min_temp, anneal_every=anneal_every,\n",
    "                        hard=hard, hard_rate=hard_rate, iteration=0)\n",
    "        super(SimulatedAnnealing, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        if closure is None:\n",
    "            raise Exception(\"loss closure is required to do SA\")\n",
    "\n",
    "        loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # the sampler samples randomness\n",
    "            # that is used in optimizations\n",
    "            sampler = group['sampler']\n",
    "\n",
    "            # clone all of the params to keep in case we need to swap back\n",
    "            cloned_params = [p.clone() for p in group['params']]\n",
    "\n",
    "            for p in group['params']:\n",
    "                # anneal tau if it matches the requirements\n",
    "                if group['iteration'] > 0 and group['iteration'] % group['anneal_every'] == 0:\n",
    "                    if not group['hard']:\n",
    "                        # smoother annealing: consider using this over hard annealing\n",
    "                        rate = -group['anneal_rate'] * group['iteration']\n",
    "                        group['tau'] = np.maximum(group['tau0'] * np.exp(rate),\n",
    "                                                  group['min_temp'])\n",
    "                    else:\n",
    "                        # hard annealing\n",
    "                        group['tau'] = np.maximum(group['hard_rate'] * group['tau'],\n",
    "                                                  group['min_temp'])\n",
    "\n",
    "                random_perturbation = group['sampler'].sample(p.data.size())\n",
    "                p.data = p.data / torch.norm(p.data)\n",
    "                p.data.add_(random_perturbation)\n",
    "                group['iteration'] += 1\n",
    "\n",
    "            # re-evaluate the loss function with the perturbed params\n",
    "            # if we didn't accept the new params, then swap back and return\n",
    "            loss_perturbed = closure()\n",
    "            final_loss, is_swapped_back = self.anneal(loss, loss_perturbed, group['tau'])\n",
    "            if is_swapped_back:\n",
    "                for p, pbkp in zip(group['params'], cloned_params):\n",
    "                    p.data = pbkp.data\n",
    "\n",
    "            return final_loss \n",
    "\n",
    "\n",
    "    def anneal(self, loss, loss_perturbed, tau):\n",
    "        '''returns loss, is_new_loss'''\n",
    "        def acceptance_prob(old, new, temp):\n",
    "            return torch.exp((old - new)/(temp))\n",
    "\n",
    "        if loss_perturbed.data < loss.data:\n",
    "#             print(\"old = \", loss.data, \"| pert = \", loss_perturbed.data, \" | tau = \", tau)\n",
    "            return loss_perturbed, False\n",
    "        else:\n",
    "            # evaluate the metropolis criterion\n",
    "            ap = acceptance_prob(loss, loss_perturbed, tau)\n",
    "            random = np.random.rand()\n",
    "            print(\"old = \", loss.data, \"| new = \", loss_perturbed.data,\n",
    "                  \" | ap = \", ap.data, \" | tau = \", tau, \" | r = \", random)\n",
    "            \n",
    "            if ap.data > random:\n",
    "                return loss_perturbed, False\n",
    "\n",
    "            return loss, True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    y_pred_train = net(X_train_i)\n",
    "    loss = CWC(y_pred_train, y_train_i)\n",
    "    return loss\n",
    "\n",
    "t_loss = []\n",
    "nets = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    net = Net(X_train[i].shape[1])\n",
    "\n",
    "    sampler = GaussianSampler(mu=0, sigma=1) #sampler = UniformSampler(minval=-0.5, maxval=0.5)\n",
    "    optimizer = SimulatedAnnealing(net.parameters(), sampler=sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train_i = X_train[i].to(device)\n",
    "    y_train_i = y_train[i].to(device)\n",
    "    X_test_i = X_test[i].to(device)\n",
    "    y_test_i = y_test[i].to(device)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    ite = []\n",
    "    loss_all = []\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        y_pred_train = net(X_train_i)\n",
    "        y_pred_train = torch.squeeze(y_pred_train)\n",
    "        train_loss = CWC(y_pred_train, y_train_i)\n",
    "        train_loss = train_loss.to(device)\n",
    "\n",
    "\n",
    "        ite = np.append(ite, epoch)\n",
    "        loss_all = np.append(loss_all, train_loss.detach().numpy()) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "    \n",
    "    t_loss.append(loss_all)\n",
    "    nets.append(net)\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_test): \n",
    "  return sqrt(((y_pred-y_test)**2).mean())\n",
    "\n",
    "RMSE_all = []\n",
    "CRPS_all = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    \n",
    "    net = nets[i]\n",
    "    \n",
    "    y_pred_train = net(X_train[i])\n",
    "    y_pred_train= y_pred_train.detach().numpy()\n",
    "    \n",
    "    y_train_i = y_train[i].cpu()\n",
    "    y_train_i = y_train_i.detach().numpy()\n",
    "    \n",
    "    \n",
    "    # For multi-step ahead prediction\n",
    "    y_45_ = net(X_test[i][0].unsqueeze(0)).detach().numpy()\n",
    "    y_45 = ((y_45_.T[0] + y_45_.T[1]) / 2)[0]\n",
    "    y_30_ = net(X_test[i][1].unsqueeze(0)).detach().numpy()\n",
    "    y_30 = ((y_30_.T[0] + y_30_.T[1]) / 2)[0]\n",
    "    y_15_ = net(X_test[i][2].unsqueeze(0)).detach().numpy()\n",
    "    y_15 = ((y_15_.T[0] + y_15_.T[1]) / 2)[0]\n",
    "    for j in range(3, X_test[i].shape[0]):\n",
    "        X_test[i][j][-3] = torch.tensor(y_45)\n",
    "        X_test[i][j][-2] = torch.tensor(y_30)\n",
    "        X_test[i][j][-1] = torch.tensor(y_15)\n",
    "        y_pred_j_ = net(X_test[i][j].unsqueeze(0)).detach().numpy()\n",
    "        y_pred_j = ((y_pred_j_.T[0] + y_pred_j_.T[1]) / 2)[0]\n",
    "        y_45 = y_30\n",
    "        y_30 = y_15\n",
    "        y_15 = y_pred_j\n",
    "    # end of multi-step ahead\n",
    "    \n",
    "    y_pred_test = net(X_test[i])\n",
    "    y_pred_test= y_pred_test.detach().numpy()\n",
    "    y_test_i = y_test[i].cpu()\n",
    "    y_test_i = y_test_i.detach().numpy()\n",
    "    \n",
    "    upper_train = y_pred_train.T[0]\n",
    "    lower_train = y_pred_train.T[1]\n",
    "    \n",
    "    upper = y_pred_test.T[0]\n",
    "    lower = y_pred_test.T[1]\n",
    "    \n",
    "    real_y_train = y_scaler[i].inverse_transform(y_train_i.reshape(-1, 1))\n",
    "    real_y_test = y_scaler[i].inverse_transform(y_test_i.reshape(-1, 1))\n",
    "    \n",
    "    upper_train = y_scaler[i].inverse_transform(upper_train.reshape(-1, 1))\n",
    "    lower_train = y_scaler[i].inverse_transform(lower_train.reshape(-1, 1))\n",
    "    \n",
    "    upper = y_scaler[i].inverse_transform(upper.reshape(-1, 1))\n",
    "    lower = y_scaler[i].inverse_transform(lower.reshape(-1, 1))\n",
    "    \n",
    "    real_y_test = real_y_test.flatten()\n",
    "    real_y_train = real_y_train.flatten()\n",
    "    \n",
    "    lower_train = lower_train.flatten()\n",
    "    upper_train = upper_train.flatten()\n",
    "    \n",
    "    lower = lower.flatten()\n",
    "    upper = upper.flatten()\n",
    "    \n",
    "    for j in range(len(lower)):\n",
    "        if lower[j]<10e-6:\n",
    "            lower[j]=0\n",
    "            \n",
    "    mean = (upper+lower)/2\n",
    "    std = (mean - lower)/1.96\n",
    "    \n",
    "    # Deterministic metrics\n",
    "    MAE = mean_absolute_error(real_y_test, mean)\n",
    "    RMSE = mean_squared_error(real_y_test, mean, squared=False)\n",
    "    MBE = np.mean(mean - real_y_test)\n",
    "    print(f'MAE: {MAE:.3f}')\n",
    "    print(f'RMSE: {RMSE:.3f}')\n",
    "    print(f'MBE: {MBE:.3f}')\n",
    "    \n",
    "    # Probabilistic metrics\n",
    "    PICP = PICP_func(real_y_test, lower, upper)\n",
    "    PINAW = PINAW_func(real_y_test, lower, upper)\n",
    "    C = prscore.crps_gaussian(real_y_test, mu=mean, sig=std)\n",
    "    CRPS = C.mean()\n",
    "    print(f'PICP: {PICP:.3f}')\n",
    "    print(f'PINAW: {PINAW:.3f}')\n",
    "    print(f'CRPS: {CRPS:.3f}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
